{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Domain Risk Training and Evaluation\n",
        "\n",
        "This notebook trains the domain risk model and evaluates PR/confusion metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import PrecisionRecallDisplay, confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, roc_curve\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sentineldns.features.domain_features import build_domain_feature_matrix\n",
        "from sentineldns.models.domain_risk import train_domain_risk_model\n",
        "from sentineldns.models.export import export_joblib, export_metadata\n",
        "\n",
        "csv_path = Path('../data/processed/labeled_domains.csv')\n",
        "artifacts_dir = Path('../data/artifacts/domain_risk')\n",
        "df = pd.read_csv(csv_path)\n",
        "print('rows:', len(df))\n",
        "print(df['label'].value_counts())\n",
        "print(df.groupby('label')['domain'].nunique())\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "domains = df['domain'].astype(str).tolist()\n",
        "y = df['label'].astype(int).to_numpy()\n",
        "\n",
        "X, vectorizer, _ = build_domain_feature_matrix(domains)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = LogisticRegression(solver='liblinear', class_weight='balanced', max_iter=500, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "probs = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, probs)\n",
        "\n",
        "fpr, _, thresholds = roc_curve(y_test, probs)\n",
        "finite = [(fp, thr) for fp, thr in zip(fpr, thresholds, strict=True) if np.isfinite(thr)]\n",
        "candidates = [thr for fp, thr in finite if fp <= 0.01]\n",
        "threshold = max(candidates) if candidates else 0.9\n",
        "pred = (probs >= threshold).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_test, pred)\n",
        "tp = cm[1, 1]\n",
        "fp_count = cm[0, 1]\n",
        "fn = cm[1, 0]\n",
        "precision_at_threshold = tp / max(tp + fp_count, 1)\n",
        "recall_at_threshold = tp / max(tp + fn, 1)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "PrecisionRecallDisplay(precision=precision, recall=recall).plot(ax=axes[0])\n",
        "axes[0].set_title('PR Curve (Domain Risk)')\n",
        "ConfusionMatrixDisplay(cm).plot(ax=axes[1], colorbar=False)\n",
        "axes[1].set_title(f'Confusion Matrix @ threshold={threshold:.4f}')\n",
        "plt.tight_layout()\n",
        "\n",
        "print({'threshold': threshold, 'precision': precision_at_threshold, 'recall': recall_at_threshold})\n",
        "\n",
        "artifacts_dir.mkdir(parents=True, exist_ok=True)\n",
        "export_joblib(artifacts_dir / 'model.joblib', model)\n",
        "export_joblib(artifacts_dir / 'vectorizer.joblib', vectorizer)\n",
        "export_metadata(\n",
        "    artifacts_dir / 'metadata.json',\n",
        "    {\n",
        "        'model_version': pd.Timestamp.utcnow().strftime('%Y%m%dT%H%M%SZ'),\n",
        "        'train_rows': int(len(y_train)),\n",
        "        'test_rows': int(len(y_test)),\n",
        "        'threshold': float(threshold),\n",
        "        'target_fpr': 0.01,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Also run package training path for parity with CLI/service behavior.\n",
        "metrics = train_domain_risk_model(csv_path)\n",
        "metrics"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "47558ddd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# False-positive / true-positive analysis\n",
        "# Rebuild the exact test split indices so we can inspect domain-level outcomes.\n",
        "index_df = df.reset_index().rename(columns={'index': 'row_id'})\n",
        "idx = index_df['row_id'].to_numpy()\n",
        "_, idx_test, _, _ = train_test_split(idx, y, test_size=0.25, random_state=42, stratify=y)\n",
        "analysis_df = df.iloc[idx_test].copy().reset_index(drop=True)\n",
        "analysis_df['prob'] = probs\n",
        "analysis_df['pred'] = pred\n",
        "\n",
        "false_positives = analysis_df[(analysis_df['label'] == 0) & (analysis_df['pred'] == 1)].sort_values('prob', ascending=False)\n",
        "true_positives = analysis_df[(analysis_df['label'] == 1) & (analysis_df['pred'] == 1)].sort_values('prob', ascending=False)\n",
        "\n",
        "print('Top false positives')\n",
        "display(false_positives[['domain', 'prob', 'source']].head(20))\n",
        "print('Top true positives')\n",
        "display(true_positives[['domain', 'prob', 'source']].head(20))"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "ffa734e8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}